# study-materials
NLP study materials

## 공지

1. 세미나 시작 전까지 발제문(자유롭게 자신이 해당 주제에 대해 공부한 요약 및 정리 글 + 코드를 공유 드라이브에 올려주세요.)

2. 세미나 발표는 30분 내외로 해주세요.

3. 본인의 영상이 유튜브에 올라간다는 것에 동의해주세요. (광고 넣지 않음. 오로지 복습용으로만 사용)

</br>

## 발표 스케줄 : 매주 토요일 9시


| 날짜 |  발 표 자 | 논문 제목 | 설명 |
|-------|-------|-------|---|
|2020/12/12| 김정용 |A Neural Probabilistic Language Model|[NPLM](https://jmlr.org/papers/volume3/tmp/bengio03a.pdf)|
|2020/12/12| 민경은 |A Neural Probabilistic Language Model|[NPLM](https://jmlr.org/papers/volume3/tmp/bengio03a.pdf)|
|2020/12/19| 우유란 |Distributed Representations of Words and Phrases and their Compositionality|[word2vec](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)|
|2021/01/02| 최상혁 |GloVe: Global Vectors for Word Representation|[glove](https://nlp.stanford.edu/pubs/glove.pdf)|
|2021/01/02| 정재영 |Neural Machine Translation by Jointly Learning to Align and Translate|[seq2seq with attention](https://arxiv.org/abs/1409.0473)|
|2021/01/09| 이화정 |Attention Is All You Need|[Transfomer](https://arxiv.org/abs/1706.03762)|
|2021/01/16| 최상혁 |BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|[BERT](https://arxiv.org/abs/1810.04805)|
|2021/01/16| 민경은 |BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|[BERT](https://arxiv.org/abs/1810.04805)|
|2021/01/30| 정재영 |SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS|[Graph Convolutional Network](https://arxiv.org/abs/1609.02907)|
|2021/02/06| 김민우 |ELECTRA_ Pre-training Text Encoders as Discriminators Rather Than Generators|[ELECTRA](https://arxiv.org/abs/2003.10555)|
|2021/02/20| 우유란 |Language Models are Unsupervised Multitask Learners|[GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)|  
|2021/02/27| 정재영 |Graph Attention Networks|[GAT](https://arxiv.org/abs/1710.10903)|
|2021/03/05| 최상혁 |Coreferential Reasoning Learning for Language Representation|[CorefBERT](https://arxiv.org/abs/2004.06870)|
|2021/03/13| 이화정 |Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context|[Transformer-XL](https://arxiv.org/abs/1901.02860)|
|2021/03/27| 우유란 |Get To The Point: Summarization with Pointer-Generator Networks|[Pointer-Generator Networks](https://arxiv.org/abs/1704.04368)|
|2021/04/03| 최상혁 |A Persona-Based Neural Conversation Model|[Persona-Based](https://arxiv.org/abs/1603.06155)|
|2021/04/10| 정재영 |Neural Logic Reasoning|[LINN](https://arxiv.org/abs/2008.09514)|
|2021/04/17| 우유란 |A Hierarchical Structured Self-Attentive Model for Extractive Document Summarization |[HSSAS](https://arxiv.org/abs/1805.07799)|
|2021/05/01| 정재영 |Survey on Intelligent Chatbots: State-of-the-Art and Future Research Directions |[survey](https://www.researchgate.net/publication/333931397_Survey_on_Intelligent_Chatbots_State-of-the-Art_and_Future_Research_Directions)|
|2021/05/08| 이화정 |ALBERT: A Lite BERT for Self-supervised Learning of Language Representations  |[ALBERT](https://arxiv.org/abs/1909.11942)|
|2021/05/15| 우유란 |FedNLP: A Research Platform for Federated Learning in Natural Language Processing  |[FedNLP](https://arxiv.org/abs/2104.08815)|
|2021/05/22| 정재영 |Are Pre-trained Convolutions Better than Pre-trained Transformers?  |[Pretrained-Convolution](https://arxiv.org/abs/2105.03322)|
|2021/05/29| 김정용|Deciphering Undersegmented Ancient Scripts Using Phonetic Prior  |[Deciphering Ancient Scripts](https://arxiv.org/abs/2010.11054)|
|2021/06/05| 정재영 |Pay attention to MLPs  |[gMLP](https://arxiv.org/abs/2105.08050)|
## 방법 

1. 매주 동일 논문 읽고 발제문 제출
2. 한 명씩 돌아가면서 발표
3. 발표한 영상 유튜브 업로드
  - 가능한 한 발표자가 영상 만들기 (zoom으로 방장이 녹화 시 연결 불량 문제로 끊김 부분이 함께 녹화되는 경우가 있다) 
4. 자신의 발표 차례가 아니어도 발표 영상을 만드신 분은 언제든 공유 환영.  

     
 
// 실명 공개, 자기소개 간단하게.  
  카톡방 이름변경. 카톡방에서 자기소개. 
